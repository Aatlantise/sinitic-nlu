{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-01T05:54:48.217033Z",
     "start_time": "2025-08-01T05:54:48.207542Z"
    }
   },
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast, BertForTokenClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:02:51.688276Z",
     "start_time": "2025-08-01T15:02:50.402751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset('R5dwMg/foodiereview_yue', split='train')\n",
    "# dataset = load_from_disk('data/nlptea_dataset')['train']"
   ],
   "id": "867ce78a3b6af3b7",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:02:57.769668Z",
     "start_time": "2025-08-01T15:02:57.156959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"hon9kon9ize/bert-base-cantonese\")\n",
    "# model = BertForMaskedLM.from_pretrained(\"models/canto-pretrain/checkpoint-27000\")\n",
    "# model = BertForMaskedLM.from_pretrained(\"bert-base-chinese\")\n",
    "# model = BertForTokenClassification.from_pretrained(\"bert-base-chinese\", num_labels=2) \n",
    "# model = BertForTokenClassification.from_pretrained(\"models/canto-pretrain/checkpoint-27000\", num_labels=2)\n",
    "model = BertForMaskedLM.from_pretrained(\"hon9kon9ize/bert-base-cantonese\")"
   ],
   "id": "474df68ca6c46900",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:02:58.381744Z",
     "start_time": "2025-08-01T15:02:58.371374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "    texts = [item['text'] for item in batch]\n",
    "    return tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "def evaluate_model_batched(model, tokenizer, dataset, batch_size=8, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=lambda b: collate_fn(b, tokenizer))\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = input_ids.clone()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item() * input_ids.size(0)  # Multiply by batch size\n",
    "            total_samples += input_ids.size(0)\n",
    "\n",
    "    average_loss = total_loss / total_samples\n",
    "    return average_loss\n"
   ],
   "id": "acecb853fbdaa8a2",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T15:08:53.735122Z",
     "start_time": "2025-08-01T15:02:58.965217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "average_loss = evaluate_model_batched(model, tokenizer, dataset, batch_size=8, device='mps')\n",
    "print(f\"Average Loss for BERT Model: {average_loss}\")"
   ],
   "id": "ce2312aaa6da107d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 407/720 [05:54<04:32,  1.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[84]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m average_loss = \u001B[43mevaluate_model_batched\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mmps\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mAverage Loss for BERT Model: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maverage_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[83]\u001B[39m\u001B[32m, line 27\u001B[39m, in \u001B[36mevaluate_model_batched\u001B[39m\u001B[34m(model, tokenizer, dataset, batch_size, device)\u001B[39m\n\u001B[32m     24\u001B[39m         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n\u001B[32m     25\u001B[39m         loss = outputs.loss\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m         total_loss += \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m * input_ids.size(\u001B[32m0\u001B[39m)  \u001B[38;5;66;03m# Multiply by batch size\u001B[39;00m\n\u001B[32m     28\u001B[39m         total_samples += input_ids.size(\u001B[32m0\u001B[39m)\n\u001B[32m     30\u001B[39m average_loss = total_loss / total_samples\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T05:57:13.342349Z",
     "start_time": "2025-08-01T05:57:13.335077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"cantonese_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens and non-target labels to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx] if label[word_idx] == 1 else -100)\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def evaluate_token_class():\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "    )\n",
    "    for example in tqdm(tokenized_dataset):\n",
    "        inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True, return_tensors=\"pt\").to('mps')\n",
    "        labels = torch.tensor(example[\"labels\"]).unsqueeze(0).to('mps')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            # check if loss.item() is a NaN\n",
    "            if loss.item() is not None and not np.isnan(loss.item()):\n",
    "                total_loss += loss.item()\n",
    "                total_tokens += inputs.input_ids.size(1)\n",
    "        \n",
    "\n",
    "    average_loss = total_loss / total_tokens\n",
    "    return average_loss"
   ],
   "id": "3607abbe92c34fb6",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T05:58:26.203830Z",
     "start_time": "2025-08-01T05:57:13.575325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "average_loss = evaluate_token_class()\n",
    "print(f\"Average Loss for Token Classification: {average_loss}\")"
   ],
   "id": "bb0dbd2161f2e56d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2062/2062 [01:12<00:00, 28.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Token Classification: 0.8704476618805232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1eb86a6125a8983b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
