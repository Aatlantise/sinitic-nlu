{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T01:07:38.224719Z",
     "start_time": "2025-07-31T01:07:27.722105Z"
    }
   },
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "ds = load_dataset(\"R5dwMg/zh-wiki-yue-long\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yorky/PycharmProjects/sinitic-nlu/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T21:27:08.639629Z",
     "start_time": "2025-07-04T21:27:08.631511Z"
    }
   },
   "cell_type": "code",
   "source": "ds['train'][0]",
   "id": "d0b7472eff76d630",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '少年阿寶·尼爾，係自護軍攻擊佢居住嘅宇宙都市嗰時，為咗保衞朋友而上咗聯邦第一架實戰用MS「高達」而捲入咗戰爭，故事就係以阿寶為中心，描佢跟住高達嘅母艦白色基地，經歷種種困難而成長嘅故事，重被認定為少見嘅「新人類」。'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T03:25:33.538101Z",
     "start_time": "2025-08-15T03:25:31.673092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizerFast, BertModel, BertForMaskedLM\n",
    "model_name = \"hon9kon9ize/bert-base-cantonese\"\n",
    "# model_name = \"./canto-pretrain/checkpoint-27000\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "model.save_pretrained('models/bert-base-cantonese')\n",
    "tokenizer.save_pretrained('models/bert-base-cantonese')"
   ],
   "id": "7c7c4909f0d5b050",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/bert-base-cantonese/tokenizer_config.json',\n",
       " 'models/bert-base-cantonese/special_tokens_map.json',\n",
       " 'models/bert-base-cantonese/vocab.txt',\n",
       " 'models/bert-base-cantonese/added_tokens.json',\n",
       " 'models/bert-base-cantonese/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T07:25:06.063655Z",
     "start_time": "2025-07-20T07:25:05.238048Z"
    }
   },
   "cell_type": "code",
   "source": "BertTokenizerFast.from_pretrained('hon9kon9ize/bert-base-cantonese').save_pretrained(\"./hon9kon9ize-canto\")",
   "id": "8a41d4c55e766eda",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./hon9kon9ize-canto/tokenizer_config.json',\n",
       " './hon9kon9ize-canto/special_tokens_map.json',\n",
       " './hon9kon9ize-canto/vocab.txt',\n",
       " './hon9kon9ize-canto/added_tokens.json',\n",
       " './hon9kon9ize-canto/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T03:40:36.459701Z",
     "start_time": "2025-07-09T03:40:35.441823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "text = '[MASK]學校，老師俾咗份卷我。'\n",
    "# text = '拔萃有個大仆街。' #拔萃有個大仆街。\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "top_k = 5\n",
    "for i in range(mask_token_index.shape[0]):\n",
    "    mask_logits = logits[0, mask_token_index[i], :]\n",
    "    top_tokens = torch.topk(mask_logits, top_k, dim=0).indices.tolist()\n",
    "    for token_id in top_tokens:\n",
    "        token = tokenizer.decode([token_id])\n",
    "        print(f\"Predicted token: {token}\")"
   ],
   "id": "d2098d9f4f0b494b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: 到\n",
      "Predicted token: 返\n",
      "Predicted token: 去\n",
      "Predicted token: 回\n",
      "Predicted token: 在\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T06:58:57.881720Z",
     "start_time": "2025-07-20T06:58:57.873163Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.tokenize('然而，Enciclopedia Libre嘅用戶唔排除喺未來重新合併嘅可能性，同埋希望繼續同維基百科保持聯繫。呢場紛爭亦引起咗關於非英文維基百科版本嘅角色嘅廣泛討論，並且直接導致非英文維基百科嘅幾項重大改革。')",
   "id": "cf36527b340c662",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['然',\n",
       " '而',\n",
       " '，',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '嘅',\n",
       " '用',\n",
       " '戶',\n",
       " '唔',\n",
       " '排',\n",
       " '除',\n",
       " '喺',\n",
       " '未',\n",
       " '來',\n",
       " '重',\n",
       " '新',\n",
       " '合',\n",
       " '併',\n",
       " '嘅',\n",
       " '可',\n",
       " '能',\n",
       " '性',\n",
       " '，',\n",
       " '同',\n",
       " '埋',\n",
       " '希',\n",
       " '望',\n",
       " '繼',\n",
       " '續',\n",
       " '同',\n",
       " '維',\n",
       " '基',\n",
       " '百',\n",
       " '科',\n",
       " '保',\n",
       " '持',\n",
       " '聯',\n",
       " '繫',\n",
       " '。',\n",
       " '呢',\n",
       " '場',\n",
       " '紛',\n",
       " '爭',\n",
       " '亦',\n",
       " '引',\n",
       " '起',\n",
       " '咗',\n",
       " '關',\n",
       " '於',\n",
       " '非',\n",
       " '英',\n",
       " '文',\n",
       " '維',\n",
       " '基',\n",
       " '百',\n",
       " '科',\n",
       " '版',\n",
       " '本',\n",
       " '嘅',\n",
       " '角',\n",
       " '色',\n",
       " '嘅',\n",
       " '廣',\n",
       " '泛',\n",
       " '討',\n",
       " '論',\n",
       " '，',\n",
       " '並',\n",
       " '且',\n",
       " '直',\n",
       " '接',\n",
       " '導',\n",
       " '致',\n",
       " '非',\n",
       " '英',\n",
       " '文',\n",
       " '維',\n",
       " '基',\n",
       " '百',\n",
       " '科',\n",
       " '嘅',\n",
       " '幾',\n",
       " '項',\n",
       " '重',\n",
       " '大',\n",
       " '改',\n",
       " '革',\n",
       " '。']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T07:02:43.824684Z",
     "start_time": "2025-07-20T07:02:43.806246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        ds['train'][i : i + 1000]['text']\n",
    "        for i in range(0, len(ds['train']), 1000)\n",
    "    )\n",
    "training_corpus = get_training_corpus()"
   ],
   "id": "ed3bf45a4df230bc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T07:03:44.232491Z",
     "start_time": "2025-07-20T07:03:42.854561Z"
    }
   },
   "cell_type": "code",
   "source": "new_tokenizer = tokenizer.train_new_from_iterator(training_corpus, vocab_size=30000)",
   "id": "a05f8cf8862d870f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T07:03:58.140777Z",
     "start_time": "2025-07-20T07:03:58.077964Z"
    }
   },
   "cell_type": "code",
   "source": "new_tokenizer.save_pretrained(\"./canto-pretrain-tokenizer\")",
   "id": "cbe93b716910a1a3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./canto-pretrain-tokenizer/tokenizer_config.json',\n",
       " './canto-pretrain-tokenizer/special_tokens_map.json',\n",
       " './canto-pretrain-tokenizer/vocab.txt',\n",
       " './canto-pretrain-tokenizer/added_tokens.json',\n",
       " './canto-pretrain-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T01:13:17.641365Z",
     "start_time": "2025-07-31T01:13:17.627421Z"
    }
   },
   "cell_type": "code",
   "source": "ds = load_from_disk('data/nlptea_dataset')['train']",
   "id": "b2c81bcfb368515c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T01:13:18.023651Z",
     "start_time": "2025-07-31T01:13:18.014304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example = ds[0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ],
   "id": "b67614179d1413a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '仍',\n",
       " '記',\n",
       " '得',\n",
       " '小',\n",
       " '學',\n",
       " '下',\n",
       " '課',\n",
       " '的',\n",
       " '時',\n",
       " '候',\n",
       " '，',\n",
       " '我',\n",
       " '總',\n",
       " '愛',\n",
       " '跟',\n",
       " '表',\n",
       " '弟',\n",
       " '到',\n",
       " '草',\n",
       " '推',\n",
       " '裏',\n",
       " '捉',\n",
       " '蠶',\n",
       " '蟲',\n",
       " '，',\n",
       " '每',\n",
       " '當',\n",
       " '打',\n",
       " '開',\n",
       " '葉',\n",
       " '子',\n",
       " '時',\n",
       " '看',\n",
       " '到',\n",
       " '蟲',\n",
       " '子',\n",
       " '，',\n",
       " '心',\n",
       " '中',\n",
       " '總',\n",
       " '有',\n",
       " '無',\n",
       " '限',\n",
       " '的',\n",
       " '喜',\n",
       " '悅',\n",
       " '。',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T01:13:19.129460Z",
     "start_time": "2025-07-31T01:13:19.126565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"cantonese_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "id": "9b10f94d69d09d61",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T01:13:20.107869Z",
     "start_time": "2025-07-31T01:13:19.927230Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)",
   "id": "d5a954bf2252f5b3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2062/2062 [00:00<00:00, 11934.36 examples/s]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T09:07:16.726745Z",
     "start_time": "2025-08-21T09:07:15.138273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from simalign import SentenceAligner\n",
    "myaligner = SentenceAligner(model=\"bert-base-chinese\", matching_methods=\"m\", device=\"mps\")\n",
    "src_sentence = [\"全\",\"部\",\"同\",\"我\",\"企\",\"起\",\"身\"]\n",
    "trg_sentence = [\"全\",\"部\",\"跟\",\"我\",\"站\",\"起\",\"来\"]\n",
    "alignments = myaligner.get_word_aligns(src_sentence, trg_sentence)\n",
    "\n",
    "for matching_method in alignments:\n",
    "    print(matching_method, \":\", alignments[matching_method])"
   ],
   "id": "d8fe104252d61acb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 05:07:16,674 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-chinese\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mwmf : [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6)]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "694ff2acb15d7656"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
